{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155b718b",
   "metadata": {},
   "source": [
    "# Corpus paralelo Náhuatl (ncx) – Español (es) desde JW.org\n",
    "\n",
    "**Autor:** preparado para Samuel Pérez Zistecatl  \n",
    "**Objetivo:** Extraer y alinear verso por verso capítulos bíblicos publicados en JW.org en Náhuatl (del centro, `ncx`) y Español (`es`), produciendo:\n",
    "- `corpus_ncx_es.csv` (tabla con columnas: `lang, libro, capitulo, versiculo, texto`)\n",
    "- `parallel_ncx_es.jsonl` (líneas con `{src, tgt, libro, capitulo, versiculo}`)\n",
    "- `metadata.json` (resumen, avisos de desalineación, etc.)\n",
    "\n",
    "> ⚠️ **Uso responsable**: Revisa los **Términos de uso** y `robots.txt` de JW.org. Ejecuta con pausas (`sleep`) y solo con fines académicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825e9f07",
   "metadata": {},
   "source": [
    "## 0) Dependencias\n",
    "Ejecuta esta celda **una vez** para instalar dependencias en tu entorno local (si hace falta). En algunos entornos puede ser `!pip` en vez de `%pip`.\n",
    "\n",
    "```bash\n",
    "%pip install requests beautifulsoup4 tqdm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435c033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Importaciones y constantes\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE = \"https://www.jw.org\"\n",
    "\n",
    "# Prefijos observados públicamente\n",
    "LANG_PREFIX = {\n",
    "    \"es\":  \"es/biblioteca/biblia/nwt/libros\",\n",
    "    \"ncx\": \"ncx/amatlajkuilolmej/biblia/nwt/libros\",\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Corpus académico; contacto@example.com)\",\n",
    "    \"Accept-Language\": \"es-ES,es;q=0.9\",\n",
    "}\n",
    "\n",
    "class FetchError(Exception):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217641e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Utilidades de red y descubrimiento\n",
    "def get(url: str, session: requests.Session, timeout: int = 20) -> requests.Response:\n",
    "    \"\"\"GET con control de errores y reintentos simples.\"\"\"\n",
    "    for intento in range(3):\n",
    "        try:\n",
    "            resp = session.get(url, headers=HEADERS, timeout=timeout)\n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "            if resp.status_code == 404:\n",
    "                return resp\n",
    "            time.sleep(1.0 + intento)\n",
    "        except requests.exceptions.RequestException:\n",
    "            time.sleep(1.0 + intento)\n",
    "    raise FetchError(f\"No se pudo obtener {url} tras reintentos.\")\n",
    "\n",
    "def libros_disponibles(lang: str, session: requests.Session) -> list:\n",
    "    \"\"\"\n",
    "    Descubre slugs de libros desde la página /libros/ del idioma dado.\n",
    "    Devuelve una lista de slugs (p.ej., 'mateo', 'marcos', 'lucas', ...).\n",
    "    \"\"\"\n",
    "    pref = LANG_PREFIX[lang].rstrip(\"/\")\n",
    "    url = f\"{BASE}/{pref}/\"\n",
    "    resp = get(url, session)\n",
    "    if resp.status_code != 200:\n",
    "        raise FetchError(f\"No se pudo acceder a {url} [{resp.status_code}]\")\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    slugs = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"/\"):\n",
    "            full = urljoin(BASE, href)\n",
    "        elif href.startswith(\"http\"):\n",
    "            full = href\n",
    "        else:\n",
    "            full = urljoin(url, href)\n",
    "\n",
    "        parsed = urlparse(full)\n",
    "        parts = [p for p in parsed.path.split(\"/\") if p]\n",
    "        try:\n",
    "            idx = parts.index(\"libros\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if idx + 1 < len(parts):\n",
    "            slug = parts[idx + 1]\n",
    "            if slug.isdigit():\n",
    "                continue\n",
    "            slugs.add(slug.strip().lower())\n",
    "\n",
    "    return sorted(slugs)\n",
    "\n",
    "def contar_capitulos(lang: str, libro: str, session: requests.Session, max_busqueda: int = 200) -> int:\n",
    "    \"\"\"Cuenta capítulos probando /<libro>/<n>/ hasta 404.\"\"\"\n",
    "    pref = LANG_PREFIX[lang].rstrip(\"/\")\n",
    "    for cap in range(1, max_busqueda + 1):\n",
    "        url = f\"{BASE}/{pref}/{libro}/{cap}/\"\n",
    "        resp = get(url, session)\n",
    "        if resp.status_code == 404:\n",
    "            return cap - 1\n",
    "    return max_busqueda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d6cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Limpieza y extracción de versos\n",
    "def limpiar_texto(texto: str) -> str:\n",
    "    \"\"\"Normaliza espacios y remueve símbolos aislados, conserva dígitos útiles.\"\"\"\n",
    "    texto = texto.replace(\"\\u202f\", \" \").replace(\"\\xa0\", \" \")\n",
    "    texto = re.sub(r\"\\s*\\*\\s*\", \" \", texto)\n",
    "    texto = re.sub(r\"\\s*\\+\\s*\", \" \", texto)\n",
    "    texto = re.sub(r\"[ \\t]+\", \" \", texto)\n",
    "    texto = re.sub(r\"\\s*\\n\\s*\", \"\\n\", texto)\n",
    "    return texto.strip()\n",
    "\n",
    "def extraer_versos_de_capitulo(html: str) -> list:\n",
    "    \"\"\"\n",
    "    Intenta extraer un listado de versos en orden desde el contenedor principal.\n",
    "    Heurística: buscar id='bibleText' y spans/p con id tipo v1,v2,... o data-pid secuencial.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    root = soup.find(id=\"bibleText\") or soup\n",
    "\n",
    "    versos = []\n",
    "    candidatos = []\n",
    "    for tag in root.find_all([\"span\", \"p\"]):\n",
    "        tid = (tag.get(\"id\") or \"\").lower()\n",
    "        dp = (tag.get(\"data-pid\") or \"\").strip()\n",
    "        if re.match(r\"^v\\d+$\", tid) or dp.isdigit():\n",
    "            candidatos.append(tag)\n",
    "\n",
    "    if not candidatos:\n",
    "        for p in root.find_all(\"p\"):\n",
    "            txt = p.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                partes = re.split(r\"(?:(?<=\\s)|^)\\d{1,3}\\s+\", txt)\n",
    "                for t in partes:\n",
    "                    t = t.strip()\n",
    "                    if t:\n",
    "                        versos.append(t)\n",
    "    else:\n",
    "        for tag in candidatos:\n",
    "            for sub in tag.find_all([\"sup\", \"a\", \"span\"], recursive=True):\n",
    "                sub_text = (sub.get_text(\"\", strip=True) or \"\").strip()\n",
    "                if sub.name in {\"sup\"} or re.match(r\"^\\d{1,3}$\", sub_text):\n",
    "                    sub.decompose()\n",
    "            t = tag.get_text(\" \", strip=True)\n",
    "            t = limpiar_texto(t)\n",
    "            if t:\n",
    "                versos.append(t)\n",
    "\n",
    "    versos = [limpiar_texto(v) for v in versos if v and v.strip()]\n",
    "    versos = [v for v in versos if not re.fullmatch(r\"\\d{1,3}\", v)]\n",
    "    return versos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab713f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Construcción del corpus (bucle principal)\n",
    "def construir_corpus(outdir: str, sleep: float = 1.0, max_libros: int = 0) -> dict:\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    csv_path = os.path.join(outdir, \"corpus_ncx_es.csv\")\n",
    "    jsonl_path = os.path.join(outdir, \"parallel_ncx_es.jsonl\")\n",
    "    meta_path = os.path.join(outdir, \"metadata.json\")\n",
    "\n",
    "    s = requests.Session()\n",
    "\n",
    "    print(\"Descubriendo libros disponibles...\")\n",
    "    libros_ncx = libros_disponibles(\"ncx\", s)\n",
    "    libros_es = libros_disponibles(\"es\", s)\n",
    "    inter = [x for x in libros_ncx if x in set(libros_es)]\n",
    "    if max_libros and max_libros > 0:\n",
    "        inter = inter[:max_libros]\n",
    "\n",
    "    meta = {\n",
    "        \"total_pairs\": 0,\n",
    "        \"libros_interseccion\": inter,\n",
    "        \"avisos\": [],\n",
    "        \"por_libro\": {},\n",
    "    }\n",
    "\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as fcsv, open(jsonl_path, \"w\", encoding=\"utf-8\") as fjsonl:\n",
    "        w = csv.writer(fcsv)\n",
    "        w.writerow([\"lang\", \"libro\", \"capitulo\", \"versiculo\", \"texto\"])\n",
    "\n",
    "        for libro in inter:\n",
    "            caps_ncx = contar_capitulos(\"ncx\", libro, s)\n",
    "            caps_es  = contar_capitulos(\"es\",  libro, s)\n",
    "            caps = min(caps_ncx, caps_es)\n",
    "            meta[\"por_libro\"][libro] = {\"caps_ncx\": caps_ncx, \"caps_es\": caps_es, \"caps_usados\": caps}\n",
    "\n",
    "            for cap in tqdm(range(1, caps + 1), desc=f\"{libro}\", unit=\"cap\"):\n",
    "                url_ncx = f\"{BASE}/{LANG_PREFIX['ncx']}/{libro}/{cap}/\"\n",
    "                url_es  = f\"{BASE}/{LANG_PREFIX['es']}/{libro}/{cap}/\"\n",
    "\n",
    "                resp_ncx = get(url_ncx, s)\n",
    "                resp_es  = get(url_es, s)\n",
    "\n",
    "                versos_ncx = extraer_versos_de_capitulo(resp_ncx.text)\n",
    "                versos_es  = extraer_versos_de_capitulo(resp_es.text)\n",
    "\n",
    "                n, m = len(versos_ncx), len(versos_es)\n",
    "                if n == 0 or m == 0:\n",
    "                    meta[\"avisos\"].append(f\"Sin versos en {libro} {cap} (ncx={n}, es={m})\")\n",
    "                    continue\n",
    "\n",
    "                L = min(n, m)\n",
    "                if n != m:\n",
    "                    meta[\"avisos\"].append(f\"Desalineación {libro} {cap}: ncx={n}, es={m}. Truncado a {L}.\")\n",
    "\n",
    "                for i in range(L):\n",
    "                    v_idx = i + 1\n",
    "                    src = versos_ncx[i]\n",
    "                    tgt = versos_es[i]\n",
    "\n",
    "                    w.writerow([\"ncx\", libro, cap, v_idx, src])\n",
    "                    w.writerow([\"es\",  libro, cap, v_idx, tgt])\n",
    "\n",
    "                    fjsonl.write(json.dumps({\n",
    "                        \"src\": src,\n",
    "                        \"tgt\": tgt,\n",
    "                        \"libro\": libro,\n",
    "                        \"capitulo\": cap,\n",
    "                        \"versiculo\": v_idx,\n",
    "                        \"src_lang\": \"ncx\",\n",
    "                        \"tgt_lang\": \"es\",\n",
    "                    }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                    meta[\"total_pairs\"] += 1\n",
    "\n",
    "                time.sleep(sleep + random.uniform(0, 0.5))\n",
    "\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nListo. Guardado en:\\n- {csv_path}\\n- {jsonl_path}\\n- {meta_path}\")\n",
    "    print(f\"Total de pares alineados: {meta['total_pairs']}\")\n",
    "    if meta[\"avisos\"]:\n",
    "        print(\"\\nAvisos (primeros 10):\")\n",
    "        for a in meta[\"avisos\"][:10]:\n",
    "            print(\" -\", a)\n",
    "        if len(meta[\"avisos\"]) > 10:\n",
    "            print(f\"... y {len(meta['avisos'])-10} avisos más.\")\n",
    "\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ccf578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descubriendo libros disponibles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%c3%89xodo: 100%|██████████| 40/40 [13:16<00:00, 19.91s/cap]\n",
      "1-corintios: 100%|██████████| 16/16 [05:33<00:00, 20.83s/cap]\n",
      "1-juan: 100%|██████████| 5/5 [01:41<00:00, 20.30s/cap]\n",
      "1-pedro: 100%|██████████| 5/5 [01:40<00:00, 20.02s/cap]\n",
      "1-samuel: 100%|██████████| 31/31 [11:55<00:00, 23.09s/cap]\n",
      "1-tesalonicenses: 100%|██████████| 5/5 [01:48<00:00, 21.68s/cap]\n",
      "1-timoteo: 100%|██████████| 6/6 [01:59<00:00, 19.94s/cap]\n",
      "2-corintios: 100%|██████████| 13/13 [04:49<00:00, 22.27s/cap]\n",
      "2-juan: 100%|██████████| 1/1 [00:21<00:00, 21.20s/cap]\n",
      "2-pedro: 100%|██████████| 3/3 [01:05<00:00, 21.72s/cap]\n",
      "2-samuel: 100%|██████████| 24/24 [08:53<00:00, 22.25s/cap]\n",
      "2-tesalonicenses: 100%|██████████| 3/3 [01:05<00:00, 21.77s/cap]\n",
      "2-timoteo: 100%|██████████| 4/4 [01:54<00:00, 28.58s/cap]\n",
      "3-juan: 100%|██████████| 1/1 [00:23<00:00, 23.79s/cap]\n",
      "apocalipsis: 100%|██████████| 22/22 [09:24<00:00, 25.66s/cap]\n",
      "colosenses: 100%|██████████| 4/4 [01:58<00:00, 29.58s/cap]\n",
      "deuteronomio:  85%|████████▌ | 29/34 [19:05<03:17, 39.51s/cap]\n"
     ]
    },
    {
     "ename": "FetchError",
     "evalue": "No se pudo obtener https://www.jw.org/ncx/amatlajkuilolmej/biblia/nwt/libros/deuteronomio/30/ tras reintentos.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFetchError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Ejecuta para construir el corpus\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     meta = \u001b[43mconstruir_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutdir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTDIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSLEEP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_libros\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_LIBROS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# meta\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mconstruir_corpus\u001b[39m\u001b[34m(outdir, sleep, max_libros)\u001b[39m\n\u001b[32m     35\u001b[39m url_ncx = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLANG_PREFIX[\u001b[33m'\u001b[39m\u001b[33mncx\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibro\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcap\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m url_es  = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLANG_PREFIX[\u001b[33m'\u001b[39m\u001b[33mes\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibro\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcap\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m resp_ncx = \u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_ncx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m resp_es  = get(url_es, s)\n\u001b[32m     41\u001b[39m versos_ncx = extraer_versos_de_capitulo(resp_ncx.text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, session, timeout)\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.RequestException:\n\u001b[32m     13\u001b[39m         time.sleep(\u001b[32m1.0\u001b[39m + intento)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m FetchError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo se pudo obtener \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tras reintentos.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFetchError\u001b[39m: No se pudo obtener https://www.jw.org/ncx/amatlajkuilolmej/biblia/nwt/libros/deuteronomio/30/ tras reintentos."
     ]
    }
   ],
   "source": [
    "# 5) Parámetros y ejecución\n",
    "OUTDIR = r\"C:\\Users\\Samuel Perez\\Desktop\\articulo\"\n",
    "       # Cambia la carpeta si lo prefieres\n",
    "SLEEP = 1.2                 # Segundos de espera entre capítulos (respeto al servidor)\n",
    "MAX_LIBROS = 0              # 0 = todos en intersección; p.ej., usa 2 para prueba rápida\n",
    "\n",
    "# Ejecuta para construir el corpus\n",
    "if __name__ == \"__main__\":\n",
    "    meta = construir_corpus(outdir=OUTDIR, sleep=SLEEP, max_libros=MAX_LIBROS)\n",
    "    # meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868cc7b",
   "metadata": {},
   "source": [
    "## 6) Siguientes pasos (opcional)\n",
    "- Dividir `parallel_ncx_es.jsonl` en *train/valid/test*.\n",
    "- Normalizar ortografía/tokenización para `ncx` según tus criterios.\n",
    "- Exportar a formato plano (`src.txt`/`tgt.txt`) para tu pipeline de NMT.\n",
    "- Añadir verificación automática de desalineaciones (por ejemplo, heurísticas por longitud).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4c50b-49ae-4030-bca1-192c6d2ae228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
